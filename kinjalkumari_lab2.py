# -*- coding: utf-8 -*-
"""Kinjalkumari_lab2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OFH4xVTABr2nGOJttad0oHC2OS9L2vVz
"""

import tensorflow as tf

# Load the fashion mnist dataset
(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()

# Create dictionaries for the unsupervised and supervised datasets
unsupervised_kinjalkumari = {'images': train_images[:60000]}
supervised_kinjalkumari = {'images': test_images, 'labels': test_labels}

# Store the first 60,000 data samples in unsupervised_kinjalkumari and the next 10,000 in supervised_kinjalkumari
unsupervised_kinjalkumari['images'] = train_images[:60000]
supervised_kinjalkumari['images'] = test_images[:10000]
supervised_kinjalkumari['labels'] = test_labels[:10000]

# Normalize the pixel values to a range between 0-1
unsupervised_kinjalkumari['images'] = unsupervised_kinjalkumari['images'] / 255.0
supervised_kinjalkumari['images'] = supervised_kinjalkumari['images'] / 255.0

# One-hot encode the labels using tensorflow's to_categorical method
supervised_kinjalkumari['labels'] = tf.keras.utils.to_categorical(supervised_kinjalkumari['labels'])

# Display the shape of the datasets
print("Unsupervised Images Shape:", unsupervised_kinjalkumari['images'].shape)
print("Supervised Images Shape:", supervised_kinjalkumari['images'].shape)
print("Supervised Labels Shape:", supervised_kinjalkumari['labels'].shape)

from sklearn.model_selection import train_test_split
import pandas as pd

# Set the random seed to be the last two digits of your student ID number
random_seed = 94

# Split the unsupervised dataset into training and validation sets
unsupervised_train_kinjalkumari, unsupervised_val_kinjalkumari = train_test_split(unsupervised_kinjalkumari['images'], test_size=0.05, random_state=random_seed)

# Create dataframes for the training and validation sets
unsupervised_train_kinjalkumari = unsupervised_train_kinjalkumari.reshape((unsupervised_train_kinjalkumari.shape[0], -1))
unsupervised_val_kinjalkumari = unsupervised_val_kinjalkumari.reshape((unsupervised_val_kinjalkumari.shape[0], -1))

# Discard 7,000 samples from the supervised dataset
x_supervised_kinjalkumari, _, y_supervised_kinjalkumari, _ = train_test_split(supervised_kinjalkumari['images'], supervised_kinjalkumari['labels'], test_size=0.7, random_state=random_seed)

# Split the remaining supervised dataset into training, validation, and testing sets
x_train_kinjalkumari, x_test_kinjalkumari, y_train_kinjalkumari, y_test_kinjalkumari = train_test_split(x_supervised_kinjalkumari, y_supervised_kinjalkumari, test_size=0.4, random_state=random_seed)
x_val_kinjalkumari, x_test_kinjalkumari, y_val_kinjalkumari, y_test_kinjalkumari = train_test_split(x_test_kinjalkumari, y_test_kinjalkumari, test_size=0.5, random_state=random_seed)

# Display the shape of the datasets
print("Unsupervised Train Shape:", unsupervised_train_kinjalkumari.shape)
print("Unsupervised Validation Shape:", unsupervised_val_kinjalkumari.shape)
print("X Train Shape:", x_train_kinjalkumari.shape)
print("X Validation Shape:", x_val_kinjalkumari.shape)
print("X Test Shape:", x_test_kinjalkumari.shape)
print("Y Train Shape:", y_train_kinjalkumari.shape)
print("Y Validation Shape:", y_val_kinjalkumari.shape)
print("Y Test Shape:", y_test_kinjalkumari.shape)

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

# Set image size
img_size = (28, 28)

# Build model
cnn_v1_model_kinjalkumari = Sequential([
    # Convolutional layer 1
    Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same', strides=2, input_shape=img_size+(1,)),
    
    # Convolutional layer 2
    Conv2D(filters=8, kernel_size=(3, 3), activation='relu', padding='same', strides=2),
    
    # Flatten layer
    Flatten(),
    
    # Fully connected layer
    Dense(units=100, activation='relu'),
    
    # Output layer
    Dense(units=10, activation='softmax')
])

# Compile model
cnn_v1_model_kinjalkumari.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Print summary of model
cnn_v1_model_kinjalkumari.summary()

# Fit model
batch_size = 256
epochs = 10
cnn_v1_history_kinjalkumari = cnn_v1_model_kinjalkumari.fit(x_train_kinjalkumari, y_train_kinjalkumari, batch_size=batch_size, epochs=epochs, validation_data=(x_val_kinjalkumari, y_val_kinjalkumari))

from tensorflow.keras.utils import plot_model

plot_model(cnn_v1_model_kinjalkumari, show_shapes=True)

import matplotlib.pyplot as plt

# plot training and validation accuracy
plt.plot(cnn_v1_history_kinjalkumari.history['accuracy'])
plt.plot(cnn_v1_history_kinjalkumari.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

test_loss, test_acc = cnn_v1_model_kinjalkumari.evaluate(x_test_kinjalkumari, y_test_kinjalkumari)
print('Test accuracy:', test_acc)

cnn_predictions_kinjalkumari = cnn_v1_model_kinjalkumari.predict(x_test_kinjalkumari)

from sklearn.metrics import confusion_matrix
import seaborn as sns

# create confusion matrix
cm = confusion_matrix(y_test_kinjalkumari.argmax(axis=1), cnn_predictions_kinjalkumari.argmax(axis=1))

# plot confusion matrix with seaborn
sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', xticklabels=[str(i) for i in range(10)],
            yticklabels=[str(i) for i in range(10)])
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# Set random seed
np.random.seed(48) 
# Add random noise to training set
x_train_noisy_kinjalkumari = x_train_kinjalkumari + 0.2 * tf.random.normal(shape=x_train_kinjalkumari.shape)

# Add random noise to validation set
x_val_noisy_kinjalkumari = x_val_kinjalkumari + 0.2 * tf.random.normal(shape=x_val_kinjalkumari.shape)

# Clip values between 0 and 1
x_train_noisy_kinjalkumari = tf.clip_by_value(x_train_noisy_kinjalkumari, clip_value_min=0, clip_value_max=1)
x_val_noisy_kinjalkumari = tf.clip_by_value(x_val_noisy_kinjalkumari, clip_value_min=0, clip_value_max=1)

# Plot the first 10 images from the x_val_noisy_kinjalkumari
fig, axes = plt.subplots(nrows=1, ncols=10, figsize=(20, 20))
for i in range(10):
    axes[i].imshow(x_val_noisy_kinjalkumari[i], cmap='gray')
    axes[i].axis('off')
plt.show()

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose

# Encoder Section
inputs_kinjalkumari = Input(shape=(28, 28, 1))
e_kinjalkumari = Conv2D(16, (3, 3), activation='relu', padding='same', strides=2)(inputs_kinjalkumari)
e_kinjalkumari = Conv2D(8, (3, 3), activation='relu', padding='same', strides=2)(e_kinjalkumari)

# Decoder Section
d_kinjalkumari = Conv2DTranspose(8, (3, 3), activation='relu', padding='same', strides=2)(e_kinjalkumari)
d_kinjalkumari = Conv2DTranspose(16, (3, 3), activation='relu', padding='same', strides=2)(d_kinjalkumari)
d_kinjalkumari = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(d_kinjalkumari)

# Autoencoder Model
autoencoder_kinjalkumari = Model(inputs=inputs_kinjalkumari, outputs=d_kinjalkumari)
autoencoder_kinjalkumari.compile(optimizer='adam', loss='mean_squared_error')

autoencoder_kinjalkumari.summary()

history_autoencoder = autoencoder_kinjalkumari.fit(x_train_noisy_kinjalkumari, x_train_kinjalkumari, epochs=10, batch_size=256, shuffle=True, validation_data=(x_val_noisy_kinjalkumari, x_val_kinjalkumari))

#Predict the model
autoencoder_predictions_kinjalkumari = autoencoder_kinjalkumari.predict(x_val_noisy_kinjalkumari)

fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(12, 6))
for i, ax in enumerate(axes.flat):
    # Plot image
    ax.imshow(np.squeeze(autoencoder_predictions_kinjalkumari[i]), cmap='gray')
    # Remove xticks and yticks
    ax.set_xticks([])
    ax.set_yticks([])
    # Add label to the subplot
    ax.set_xlabel(f'Image {i+1}')
# Show the plot
plt.tight_layout()
plt.show()

#1. Define the CNN model
from tensorflow.keras.layers import Dense, Flatten

#Add the encoder part of the autoencoder to the CNN model
cnn_v2_kinjalkumari = Sequential(autoencoder_kinjalkumari.layers[0:5])

#Add a fully connected layer
cnn_v2_kinjalkumari.add(Flatten())
cnn_v2_kinjalkumari.add(Dense(100, activation='relu'))

#Add output layer
cnn_v2_kinjalkumari.add(Dense(10, activation='softmax'))

#2. Compile the model
cnn_v2_kinjalkumari.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

#3. Display model summary
cnn_v2_kinjalkumari.summary()

#4. Train the model
cnn_v2_history_kinjalkumari = cnn_v2_kinjalkumari.fit(x_train_noisy_kinjalkumari, y_train_kinjalkumari, epochs=10, batch_size=256, validation_data=(x_val_noisy_kinjalkumari, y_val_kinjalkumari))

import matplotlib.pyplot as plt

plt.plot(cnn_v2_history_kinjalkumari.history['accuracy'], color='blue', label='Training Accuracy')
plt.plot(cnn_v2_history_kinjalkumari.history['val_accuracy'], color='orange', label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training Vs Validation Accuracy of Pretrained CNN Model')
plt.legend()
plt.show()

# Convert one-hot encoded labels to non-one-hot encoded format
y_test_pred = np.argmax(cnn_v2_kinjalkumari.predict(x_test_kinjalkumari), axis=-1)
y_test = np.argmax(y_test_kinjalkumari, axis=-1)

# Evaluate cnn model with test dataset
test_loss, test_acc = cnn_v2_kinjalkumari.evaluate(x_test_kinjalkumari, y_test_kinjalkumari, verbose=0)
print('Test Accuracy:', test_acc)

# Create predictions on the test dataset
cnn_predictions_kinjalkumari = cnn_v2_kinjalkumari.predict(x_test_kinjalkumari)

# Compute confusion matrix
conf_matrix = confusion_matrix(y_test, y_test_pred)

# Plot confusion matrix using seaborn
sns.set(font_scale=1.4) # Adjust font size
sns.heatmap(conf_matrix, annot=True, fmt='g', cmap="Blues")
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()

import matplotlib.pyplot as plt

plt.plot(cnn_v1_history_kinjalkumari.history['val_accuracy'], color='blue', label='Baseline Validation Accuracy')
plt.plot(cnn_v2_history_kinjalkumari.history['val_accuracy'], color='orange', label='Pretrained Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Validation Accuracy')
plt.title('Comparison of Baseline vs Pretrained Model')
plt.legend()
plt.show()



